# -*- coding: utf-8 -*-
"""areebaPro2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqEb7PB49NRZR5v5V770aHB_AS043ueJ
"""

from google.colab import files

# Prompt user to upload a folder
uploaded = files.upload()

!unzip /content/ProjecttwoData.zip

"""##Part 1: Steps 1-4 (Data Processing, Model Building, Training, and Evaluation)"""

# Importing required libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Data Processing

# Set the image size and batch size
IMG_WIDTH, IMG_HEIGHT = 500, 500
BATCH_SIZE = 32

# Define data directories
train_dir = '/content/Data/train'
validation_dir = '/content/Data/valid'
test_dir = '/content/Data/test'

# Creating train and validation datasets from directories
train_dataset = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=(IMG_WIDTH, IMG_HEIGHT),
    batch_size=BATCH_SIZE,
    label_mode='categorical'  # for multi-class classification
)

validation_dataset = tf.keras.utils.image_dataset_from_directory(
    validation_dir,
    image_size=(IMG_WIDTH, IMG_HEIGHT),
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

# Data augmentation for training data
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),         # Rescaling images
    tf.keras.layers.RandomFlip("horizontal"),  # Randomly flip images horizontally
    tf.keras.layers.RandomRotation(0.1),       # Randomly rotate images
    tf.keras.layers.RandomZoom(0.1)            # Randomly zoom images
])

# Only rescaling for validation data
validation_dataset = validation_dataset.map(lambda x, y: (x / 255.0, y))

# Apply data augmentation to the training dataset
train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))

# Step 2: Neural Network Architecture Design

# Define the model
model = Sequential()

# Convolutional layers with MaxPooling and Dropout
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

# Flatten the results and add dense layers
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))  # 3 output classes: crack, missing-head, paint-off

# Step 3: Hyperparameter Analysis
# Compile the model with chosen hyperparameters
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 4: Model Evaluation
# Train the model and store the training history
epochs = 20
history = model.fit(
    train_dataset,
    epochs=epochs,
    validation_data=validation_dataset
)

# Plotting training and validation loss and accuracy
plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy')

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss')

plt.show()

"""##Part 2: Step 5 (Testing the Model on New Images)"""

# Importing required packages for model testing
from tensorflow.keras.preprocessing import image

# Define image size
IMG_WIDTH, IMG_HEIGHT = 500, 500

# Class labels (in the same order as they were used during training)
class_labels = ['Crack', 'Missing-Head', 'Paint-Off']

# Function to load, preprocess, and predict the class of an image
def process_and_predict(img_path):
    # Load the image and resize it to match the input shape of the model
    img = image.load_img(img_path, target_size=(IMG_WIDTH, IMG_HEIGHT))
    # Convert the image to an array and normalize it
    img_array = image.img_to_array(img) / 255.0
    # Add a batch dimension to the image
    img_array = np.expand_dims(img_array, axis=0)

    # Predict the class of the image
    prediction = model.predict(img_array)
    # Find the index of the class with the highest probability
    predicted_class = np.argmax(prediction, axis=1)[0]
    confidence = np.max(prediction) * 100  # Confidence level

    return img, predicted_class, confidence
# Test images paths
test_images = [
    ('/content/Data/test/crack/test_crack.jpg','Crack'),
    ('/content/Data/test/missing-head/test_missinghead.jpg','Missing-Head'),
    ('/content/Data/test/paint-off/test_paintoff.jpg','Paint-Off')
]
# Plotting the test images with actual and predicted labels
fig, axes = plt.subplots(1, len(test_images), figsize=(15, 5))
for i, (img_path, true_label) in enumerate(test_images):
    # Process and predict each test image
    img, predicted_class, confidence = process_and_predict(img_path)

    # Display the image
    axes[i].imshow(img)
    axes[i].axis('off')

    # Set the title with the actual label, predicted label, and confidence
    predicted_label = class_labels[predicted_class]
    axes[i].set_title(f"Actual: {true_label}\nPredicted: {predicted_label}\nConfidence: {confidence:.2f}%")

# Show the plot
plt.tight_layout()
plt.show()

# Save model as a single .h5 file
model.save('model.h5')